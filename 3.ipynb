{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                  class  width  height  xmin  ymin  xmax  ymax\nfilename                                                                      \nC1_P01_N3_S3_127_67_2325_2365.JPG  pack   1000    1000   293   441   434   655\nC1_P01_N3_S3_127_67_2325_2365.JPG  pack   1000    1000   660   785   779   970\nC1_P01_N3_S3_127_67_2325_2365.JPG  pack   1000    1000   794   796   925   980\nC1_P01_N3_S3_127_67_2325_2365.JPG  pack   1000    1000   416   443   557   657\nC1_P01_N3_S3_127_67_2325_2365.JPG  pack   1000    1000   409   108   550   322\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "data_path = '.'\n",
    "train_shelf_images = '../GroceryDataset/ShelfImages/train/'\n",
    "test_shelf_images = '../GroceryDataset/ShelfImages/test/'\n",
    "product_images = '../GroceryDataset/ProductImagesFromShelves/'\n",
    "cropped_path = '../GroceryDataset/cropped/'\n",
    "img_path = '../GroceryDataset/ShelfImages/train/'\n",
    "detector_data_path = '../GroceryDataset/result/'\n",
    "\n",
    "photos = pd.read_pickle(f'{data_path}photos.pkl')\n",
    "products = pd.read_pickle(f'{data_path}products.pkl')\n",
    "\n",
    "\n",
    "N_CROP_TRIALS = 6\n",
    "CROP_SIZE = 1000\n",
    "\n",
    "\n",
    "# returns random value in [s, f]\n",
    "def rand_between(s, f):\n",
    "    if s == f:\n",
    "        return s\n",
    "    return np.random.randint(s, f)\n",
    "train_products, eval_products = [], []\n",
    "\n",
    "for img_file, is_train in photos[['file', 'is_train']].values:\n",
    "    img = cv2.imread(f'{img_path}{img_file}')\n",
    "    img_h, img_w, img_c = img.shape\n",
    "    for n in range(N_CROP_TRIALS):\n",
    "        # randomly crop square\n",
    "        c_size = rand_between(300, max(img_h, img_w))\n",
    "        x0 = rand_between(0, max(0, img_w - c_size))\n",
    "        y0 = rand_between(0, max(0, img_h - c_size))\n",
    "        x1 = min(img_w, x0 + c_size)\n",
    "        y1 = min(img_h, y0 + c_size)\n",
    "        # products totally inside crop rectangle\n",
    "        crop_products = products[(products.file == img_file) &\n",
    "                                 (products.xmin > x0) & (products.xmax < x1) &\n",
    "                                 (products.ymin > y0) & (products.ymax < y1)]\n",
    "        # no products inside crop rectangle? cropping trial failed...\n",
    "        if len(crop_products) == 0:\n",
    "            continue\n",
    "        # name the crop\n",
    "        crop_img_file = f'{img_file[:-4]}{x0}_{y0}_{x1}_{y1}.JPG'\n",
    "        # crop and reshape to CROP_SIZExCROP_SIZE or smaller\n",
    "        # keeping aspect ratio\n",
    "        crop = img[y0:y1, x0:x1]\n",
    "        h, w, c = crop.shape\n",
    "        ratio = min(CROP_SIZE/h, CROP_SIZE/w)\n",
    "        crop = cv2.resize(crop, (0,0), fx=ratio, fy=ratio)\n",
    "        crop = crop[0:CROP_SIZE, 0:CROP_SIZE]\n",
    "        h, w, c = crop.shape\n",
    "        # add crop inner products to train_products or eval_products list\n",
    "        for xmin, ymin, xmax, ymax in \\\n",
    "                crop_products[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
    "            xmin -= x0\n",
    "            xmax -= x0\n",
    "            ymin -= y0\n",
    "            ymax -= y0\n",
    "\n",
    "            xmin, xmax, ymin, ymax = [int(np.round(e * ratio))\n",
    "                                      for e in [xmin, xmax, ymin, ymax]]\n",
    "            product = {'filename': crop_img_file, 'class':'pack',\n",
    "                       'width':w, 'height':h,\n",
    "                       'xmin':xmin, 'ymin':ymin, 'xmax':xmax, 'ymax':ymax}\n",
    "            if is_train:\n",
    "                train_products.append(product)\n",
    "            else:\n",
    "                eval_products.append(product)\n",
    "        # save crop top eval or train folder\n",
    "        subpath = ['eval/', 'train/'][is_train]\n",
    "        cv2.imwrite(f'{cropped_path}{subpath}{crop_img_file}', crop)\n",
    "\n",
    "train_df = pd.DataFrame(train_products).set_index('filename')\n",
    "eval_df = pd.DataFrame(eval_products).set_index('filename')\n",
    "\n",
    "print (train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'pack':\n",
    "        return 1\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) \n",
    "            for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_records(images_path, examples, dst_file):\n",
    "    writer = tf.io.TFRecordWriter(dst_file)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, images_path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tf_records(f'{cropped_path}train/', train_df, f'{detector_data_path}train.record')\n",
    "convert_to_tf_records(f'{cropped_path}eval/', eval_df, f'{detector_data_path}eval.record')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}